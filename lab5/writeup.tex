%% You should title the file with a .tex extension (hw1.tex, for example)
\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{booktabs}


\oddsidemargin0cm
\topmargin-1.5cm     %I recommend adding these three lines to increase the
\textwidth16.5cm   %amount of usable space on the page (and save trees)
\textheight23.5cm

\newcommand{\question}[2] {\vspace{.25in} \hrule\vspace{0.5em}
\noindent{\bf #1: #2} \vspace{0.5em}
\hrule \vspace{.10in}}
\renewcommand{\part}[1] {\vspace{.10in} {\bf (#1)}}

\newcommand{\myname}{Laxman Dhulipala, Peter Xiao}
\newcommand{\myandrew}{ldhulipa, phx}
\newcommand{\myhwnum}{5}

\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}

\pagestyle{fancyplain}


\begin{document}

\medskip                        % Skip a "medium" amount of space
                                % (latex determines what medium is)
                                % Also try: \bigskip, \littleskip

\thispagestyle{plain}
\begin{center}                  % Center the following lines
{\Large Lab 5 Writeup} \\
 \vspace{2 mm}
\myname \\
\myandrew
\end{center}

For this lab we implemented several optimizations that range from function inlining
to register coalescing. Specifically, we picked the following four optimizations to focus
on :

\begin{itemize}
  \item[1.] Dead Code Elimination
  \item[2.] Function Inlining
  \item[3.] Constant Folding and Propagation
  \item[4.] Register Coalescing
\end{itemize}

In the following sections, we will describe in detail the implementation and evaluation of
each optimization.

\section{Dead Code Elimination}

We performed Neededness analysis on the three operand form to determine which
variables are actually needed for the correct resulting program.

Similar to Liveness, Neededness runs from back to front because any variable
that is needed in a successor is also needed in the current line. Operands are
neeeded only if the variable being assigned is needed. Any necessary variables
are also needed.

Neededness is performed after generating three operand abstract assembly, however, as
opposed to liveness, which is generated after two operand assembly for us. This
difference is because of effectful lines like $t \leftarrow x \slash y$ require
both $x$ and $y$ to be needed in order to execute this line. In two operand
form, it is non-trivial to determine if any operand is part of a division.

After neededness analysis is done, any code where the variable assigned to is
not needed in the next line can be removed, except for effectful operations.

In practice, we did found a small, but noticeable improvement in the benchmarks.
In the benchmarks, most of the variables are needed, so not a lot of code is
removed.

\section{Function Inlining}
The goal when performing function inlining is to inline only functions where the cost of
invoking the function (the \verb+call+ instruction, and the subsequent \verb+ret+ is very
costly). Specifically, we focus our inlining on functions which consist of a single basic
block. Examples of such functions are simple sum functions, such as
\begin{verbatim}
int sum(int x, int y){
  return x + y;
}
\end{verbatim}
or even longer functions which have no loops, and have a single return. While it's possible to inline
functions which have multiple returns, this involves replacing returns with gotos, and is
significantly less clean to implement and evaluate. Furthermore, the control flow graph of
the function we inline into becomes significantly distorted. \\
\\
We perform our inlining after we have converted our AST to an IR tree. The advantage of this is that
we have a clear idea of `code length', and can inline with a heuristic based on code length. We maintain
several properties about functions which makes inlining relatively clean. \\
\\
Firstly, for a function with $n$ arguments, the first $n$ abstract assembly instructions in our IR will
be moves from argument registers into temps. Secondly, the last instruction for a function which returns a
value will always be \verb+ret+ of the register 0, which by convention is known to be \verb+eax+. Furthermore,
at a function call, we maintain information about which $n$ temps in the calling program are the argument
temps. This allows us to remove the first $n$ lines of abstract assembly, and replace them with moves from
temps in the calling function, to new temps in the inlined function.\\
\\
To ensure that temps do not conflict, we preserve information about the maximum temp number in the calling
function. Let this be $t_{max}$. To ensure no conflicts, for all temps that remain in the function we're
inlining, we increment their temp num by $t_{max}$, and then set $t_{max}$ to be the new maximum temp num.
This computation is done within a monad, so this getting and setting of state is straightforward.\\
\\
Finally,
we map the argument temps (temps from the calling function) to the argument temps in the inlined function,
and also make sure to move the result value (a temp inside of the inlined function) to the destination temp
in the calling function. \\
\\
Evaluating this optimization was straightforward : we compiled all of tests1, and emitted output whenever
a function was inlined. With our admissibility heuristic being that a function to be inlined must have :
less than 50 lines of abstract assembly, no labels, and a single return, we found that a large portion
of the tested cases allowed functions to be inlined.\\
\\
Unfortunately, the inlining optimization did not do much good on the benchmarks (bench0, bench1). A few
functions were inlined, but the tests were focused primarily on single large functions that had fairly
complex behavior (such as mmult). Optimizing for these benchmarks meant focusing on optimizing memory
usage, and loop behavior, which we will describe at the end of the document.

\section{Constant Folding and Propagation}
For this optimization, we perform constant folding and propagation on the AST. This
was an optimization that we struggled to get working in previous labs - most probably because
of a rather misinformed view that monads were scary. In this lab, we conquered our fear 
and marched boldly where neither of us had really gone before and wrote some state monads from scratch. \\
\\
Constant propagation was implemented in this way (using a StateT). The basic idea is to 
keep a map from named variables to constants. Every time we run into an assignment of a constant 
to this named variable, we insert it into the map. Every time we pass over a control flow block, we pass
in a clean state to the block - only assignments declared within that block will be propagated. \\
\\
Similarly, when we exit from a control flow block, we abandon the current state (of the control
flow block), and restore the state that we had before entering the control flow block. 
We then remove all variables that were mutated within that 
control flow block from our map - as it may no longer be correct to replace occurences of such 
variables with the constant they map to. \\
\\
Constant folding is very straight-forward. We perform an AST-based constant folding, specifically
on expressions. The use case appeared in an earlier lab when a test tested our ability to cope with
many nested unary ops (such as bitwise negate). Observing that two bitwise negates can simply be
cancelled in the front-end, we implemented a simple constant folder to pass such tests. \\
\\
We expanded the constant folder to then take into account adding, multiplying and other simple
arithmetic operations of constants. We were also careful to avoid over-optimizing - leaving
divisions for the runtime to perform.\\
\\
In previous labs we had some trouble passing one test, namely \verb+eowyn-jpaulson-loop-stress1+.
This lab, after implementing the constant propagater, we passed it very quickly. Without
constant folding and constnat propagation, this test would be almost impossible, as it computes
a rather large constant value as a loop-termination check. Furthermore, the variables used
within the loop must be allocated to registers - if you don't constant propagate and perform
some sort of dead-code removal and register allocate naively, the first temps declared will
greedily put into registers, which then causes the loop variables to be stack-allocated,
and causes the test to take over 10 seconds to run.

\section{Register Coalescing}
We implemented register coalescing using the strategy outlined in lecture 3. To summarize 
our implementation strategy, we 
\begin{itemize}
  \item [1.] Consider all moves of the form $t \leftarrow s$.
  \item [2.] If $s$,$t$ share the same color, we continue.
  \item [3.] If $(s,t)$ is an edge in our interference graph, we continue.
  \item [6.] If we can find a $c$ s.t. $c \notin N(t), c \notin N(s)$ and 
       $c$ will not be a stack allocated color, then we squash $s$
       into $t$, and update the graph so that all vertices with $s$
       in their adjacency list now point to $t$. We update the color
       of $t$ to now be $c$, and continue. 
\end{itemize}
The register coalescing module was implemented within a monad, which made things
such as keeping a stateful interference graph and coloring map up-to-date
very straightforward. The optimization takes place after register allocation, right before instruction
selection.\\
\\

\section{Safety}

Compiling in unsafe mode gave noticable speed ups to all tests that used a lot
of shifts or array accesses. For instance, in the shift benchmark, with no
optimizations, the number of cycles drops from 134676 to 129217 in one particular
run of the benchmarks. In qsort, the number of cycles drops from 447185 to
377448.

This speed up was reduced because we optimized these checks to only one size
check, rather than two. Originally, our code would check if size of the shift is less than zero, and
greater than 31. Now, we make a single unsigned comparison with 31, and throw
an exception if it exceeds 31, exploiting the fact that any negative number is
above 31.

\end{document}
